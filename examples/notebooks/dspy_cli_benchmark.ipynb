{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSPy Benchmark: CLI Summarization\n",
    "\n",
    "This notebook benchmarks two small language models (SLMs), `phi3:mini` and `llama3:8b`, for a command-line interface (CLI) text summarization task. \n",
    "\n",
    "We will perform a grid search to evaluate the models across different temperature settings, measuring both the quality (accuracy) and speed (latency) of their summaries.\n",
    "\n",
    "**Objective:** Identify the optimal model and temperature configuration for generating concise and faithful summaries of long text passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import warnings\n",
    "\n",
    "# Suppress verbose warnings from libraries\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure Language Models\n",
    "\n",
    "We define the two local models to be benchmarked using `dspy.OllamaLocal`. This assumes you have Ollama running and have pulled both `phi3:mini` and `llama3:8b`.\n",
    "\n",
    "We also configure a more powerful model (e.g., from OpenAI) to act as the judge for our quality metric. **Remember to set your `OPENAI_API_KEY` environment variable for the evaluator to work.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the local models for the grid search\n",
    "phi3_mini = dspy.OllamaLocal(model='phi3:mini', max_tokens=1024)\n",
    "llama3_8b = dspy.OllamaLocal(model='llama3:8b', max_tokens=1024)\n",
    "\n",
    "# Configure a more powerful model to act as the evaluator for our metric\n",
    "# Make sure your OPENAI_API_KEY is set in your environment variables\n",
    "try:\n",
    "    evaluator_lm = dspy.OpenAI(model='gpt-4o-mini', max_tokens=2048, model_type='chat')\n",
    "    dspy.configure(lm=llama3_8b, rm=None, evaluator=evaluator_lm)\n",
    "    print(\"Successfully configured models. Using Llama-3-8B as default and GPT-4o-mini as evaluator.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error configuring OpenAI model: {e}\")\n",
    "    print(\"Please ensure your OPENAI_API_KEY is set as an environment variable.\")\n",
    "    # Fallback for evaluator if OpenAI key is not set\n",
    "    evaluator_lm = llama3_8b \n",
    "    dspy.configure(lm=llama3_8b, rm=None, evaluator=evaluator_lm)\n",
    "    print(\"Using Llama-3-8B as a fallback evaluator. Accuracy metric may be less reliable.\")\n",
    "\n",
    "models_to_test = {\n",
    "    \"phi3:mini\": phi3_mini,\n",
    "    \"llama3:8b\": llama3_8b\n",
    "}\n",
    "\n",
    "temperature_values = [0.0, 0.5, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Data & Summarization Signature\n",
    "\n",
    "We'll create a simple dataset of long-form text passages. In a real-world scenario, you would load a more extensive and representative dataset. \n",
    "\n",
    "The `SummarizationSignature` defines the input (`document`) and output (`summary`) for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DSPy Signature for the summarization task\n",
    "class SummarizationSignature(dspy.Signature):\n",
    "    \"\"\"Summarize the given document into a short, concise paragraph.\"\"\"\n",
    "    document = dspy.InputField(desc=\"A long text document.\")\n",
    "    summary = dspy.OutputField(desc=\"A concise summary of the document.\")\n",
    "\n",
    "# Create a small example dataset\n",
    "trainset = [\n",
    "    dspy.Example(document=\"Quantum computing is a revolutionary type of computing that leverages the principles of quantum mechanics to process information. Unlike classical computers, which use bits to represent information as either a 0 or a 1, quantum computers use qubits. Qubits can exist in a superposition of both 0 and 1 simultaneously, and they can be entangled, meaning their fates are linked even when physically separated. This allows quantum computers to perform complex calculations at speeds unattainable by classical computers. Key applications include drug discovery, materials science, financial modeling, and breaking cryptographic codes. However, building and maintaining stable quantum computers is a massive engineering challenge due to qubit decoherence, where qubits lose their quantum properties due to interaction with the environment.\").with_inputs('document'),\n",
    "    dspy.Example(document=\"The Roman Republic was a period of ancient Roman civilization that began with the overthrow of the Roman Kingdom in 509 BC and ended in 27 BC with the establishment of the Roman Empire. It was characterized by a republican form of government, where annual magistrates were elected by the citizens. The cornerstone of the Republic was the Senate, a body of elder statesmen that advised the magistrates. Society was divided into patricians (the aristocracy) and plebeians (the common citizens). For centuries, the Republic expanded its territory through conquest and alliances, eventually controlling the entire Mediterranean basin. However, internal tensions, civil wars, and the rise of powerful military leaders like Julius Caesar ultimately led to its collapse and the rise of Augustus as the first Roman Emperor.\").with_inputs('document'),\n",
    "    dspy.Example(document=\"Artificial neural networks (ANNs) are computing systems inspired by the biological neural networks that constitute animal brains. An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron that receives a signal then processes it and can signal neurons connected to it. The 'signal' at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. These networks are trained on large datasets, adjusting the connection weights to perform tasks like image recognition, natural language processing, and decision making, forming the foundation of modern deep learning.\").with_inputs('document')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Evaluation Metrics\n",
    "\n",
    "We need two key metrics:\n",
    "\n",
    "1.  **Latency:** A simple wrapper to measure the wall-clock time for generating a summary.\n",
    "2.  **Accuracy (Quality):** A more complex, LLM-based metric. We ask our `evaluator_lm` to rate the summary on two criteria: **faithfulness** (does it accurately reflect the source?) and **conciseness** (is it brief and to the point?). The scores are combined for a final quality rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latency_metric(model, data_point):\n",
    "    \"\"\"Measures the time taken by a model to make a prediction.\"\"\"\n",
    "    start_time = time.time()\n",
    "    model(document=data_point.document)\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "def accuracy_metric(gold, pred, trace=None):\n",
    "    \"\"\"Uses an LLM to evaluate the quality of a generated summary.\"\"\"\n",
    "    document = gold.document\n",
    "    summary = pred.summary\n",
    "    \n",
    "    # Use a ChainOfThought prompt to get a more reasoned evaluation\n",
    "    eval_prompt = f\"\"\"Evaluate the quality of the summary for the given document.\n",
    "    \n",
    "    **Document:**\n",
    "    {document}\n",
    "    \n",
    "    **Summary:**\n",
    "    {summary}\n",
    "    \n",
    "    **Instructions:** Please rate the summary on two criteria:\n",
    "    1. **Faithfulness:** How accurately does the summary reflect the main points of the document? (1-5, 5 is best)\n",
    "    2. **Conciseness:** How brief and to-the-point is the summary? (1-5, 5 is best)\n",
    "    \n",
    "    First, think step-by-step about the faithfulness and conciseness. Then, provide your scores in the format 'Faithfulness: <score>, Conciseness: <score>'.\"\"\"\n",
    "\n",
    "    with dspy.settings.context(lm=evaluator_lm):\n",
    "        response = dspy.Predict(dspy.Signature(\"evaluation_prompt -> evaluation_output\"))(evaluation_prompt=eval_prompt).evaluation_output\n",
    "    \n",
    "    try:\n",
    "        # Extract scores from the response\n",
    "        faithfulness_score = int(response.split(\"Faithfulness:\")[-1].split(\",\")[0].strip())\n",
    "        conciseness_score = int(response.split(\"Conciseness:\")[-1].strip())\n",
    "        # Average the scores for a final quality rating\n",
    "        return (faithfulness_score + conciseness_score) / 2.0\n",
    "    except (ValueError, IndexError):\n",
    "        # If parsing fails, return a neutral score\n",
    "        return 2.5 # Neutral score on a 1-5 scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Grid Search\n",
    "\n",
    "This is the core of the notebook. We'll iterate through each model and temperature setting, configure DSPy accordingly, and run our evaluation metrics on the dataset. The results are stored in a list for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "total_configs = len(models_to_test) * len(temperature_values)\n",
    "current_config = 0\n",
    "\n",
    "print(f\"Starting grid search across {total_configs} configurations...\")\n",
    "\n",
    "# Define a simple summarization module\n",
    "summarizer_module = dspy.Predict(SummarizationSignature)\n",
    "\n",
    "for model_name, model_lm in models_to_test.items():\n",
    "    for temp in temperature_values:\n",
    "        current_config += 1\n",
    "        print(f\"\n--- [ {current_config}/{total_configs} ] --- \")\n",
    "        print(f\"Testing Model: {model_name}, Temperature: {temp}\")\n",
    "        \n",
    "        # Configure DSPy with the current model and temperature\n",
    "        dspy.settings.configure(lm=model_lm, temperature=temp, evaluator=evaluator_lm)\n",
    "\n",
    "        total_latency = 0\n",
    "        total_accuracy = 0\n",
    "        num_examples = len(trainset)\n",
    "\n",
    "        for i, example in enumerate(trainset):\n",
    "            print(f\"  Processing example {i+1}/{num_examples}...\", end='\r')\n",
    "            # Measure latency\n",
    "            total_latency += latency_metric(summarizer_module, example)\n",
    "\n",
    "            # Measure accuracy\n",
    "            prediction = summarizer_module(document=example.document)\n",
    "            total_accuracy += accuracy_metric(example, prediction)\n",
    "        \n",
    "        avg_latency = total_latency / num_examples\n",
    "        avg_accuracy = total_accuracy / num_examples\n",
    "\n",
    "        print(f\"\n  Avg Latency: {avg_latency:.4f} seconds\")\n",
    "        print(f\"  Avg Accuracy: {avg_accuracy:.4f} / 5.0\")\n",
    "        \n",
    "        results.append({\n",
    "            'model': model_name,\n",
    "            'temperature': temp,\n",
    "            'avg_accuracy': avg_accuracy,\n",
    "            'avg_latency_s': avg_latency\n",
    "        })\n",
    "\n",
    "print(\"\nGrid search complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save and Display Ranked Results\n",
    "\n",
    "Finally, we'll convert our results into a Pandas DataFrame, rank them (prioritizing higher accuracy, then lower latency), and save the output to `benchmark_results.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Rank the results: higher accuracy is better, lower latency is better\n",
    "ranked_df = results_df.sort_values(by=['avg_accuracy', 'avg_latency_s'], ascending=[False, True])\n",
    "\n",
    "# Reset index for clean presentation\n",
    "ranked_df.reset_index(drop=True, inplace=True)\n",
    "ranked_df.index += 1 # Start index at 1 for ranking\n",
    "\n",
    "# Save to CSV\n",
    "output_filename = 'benchmark_results.csv'\n",
    "ranked_df.to_csv(output_filename, index_label='rank')\n",
    "\n",
    "print(f\"Results saved to {output_filename}\")\n",
    "\n",
    "# Display the final ranked table\n",
    "print(\"\n--- Final Ranked Results ---\")\n",
    "print(ranked_df.to_markdown(index=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
