{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Objective Bayesian Optimization for CLI Summarization\n",
    "\n",
    "This notebook advances our previous benchmarking efforts by employing Bayesian multi-objective optimization to find the best configurations for a command-line summarization tool. Instead of a simple grid search, we use `scikit-optimize` to intelligently explore the hyperparameter space.\n",
    "\n",
    "### Objectives\n",
    "We aim to find configurations that are **Pareto-optimal** across three competing objectives:\n",
    "1.  **Maximize Accuracy**: The factual correctness and quality of the summary.\n",
    "2.  **Minimize Latency**: The wall-clock time required to generate a summary.\n",
    "3.  **Minimize Token Cost**: The estimated monetary cost of the API call, based on input and output token counts.\n",
    "\n",
    "### Search Space\n",
    "The optimizer will sweep through the following hyperparameters:\n",
    "-   **Model**: `phi3:mini`, `llama3:8b` (Categorical)\n",
    "-   **Temperature**: `0.0` to `1.0` (Continuous)\n",
    "-   **Max Tokens**: `256`, `512`, `768` (Categorical)\n",
    "\n",
    "### Outcome\n",
    "The result is a 3D plot visualizing the trade-offs between our three objectives and a saved CSV file, `pareto_frontier.csv`, containing the set of most efficient configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Categorical\n",
    "\n",
    "# Suppress verbose warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure Models, Costs, and Search Space\n",
    "\n",
    "We'll configure our local models, define a cost estimation function, and lay out the hyperparameter search space for the Bayesian optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Configuration ---\n",
    "phi3_mini = dspy.OllamaLocal(model='phi3:mini')\n",
    "llama3_8b = dspy.OllamaLocal(model='llama3:8b')\n",
    "\n",
    "models_map = {\n",
    "    \"phi3:mini\": phi3_mini,\n",
    "    \"llama3:8b\": llama3_8b\n",
    "}\n",
    "\n",
    "# --- Evaluator Configuration ---\n",
    "try:\n",
    "    # Using a powerful model as a judge is preferred.\n",
    "    # Ensure OPENAI_API_KEY is set in your environment variables.\n",
    "    evaluator_lm = dspy.OpenAI(model='gpt-4o-mini', max_tokens=2048, model_type='chat')\n",
    "    print(\"Using GPT-4o-mini as the evaluator.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not initialize OpenAI evaluator: {e}\")\n",
    "    print(\"Falling back to llama3:8b as the evaluator. Quality scores may be less reliable.\")\n",
    "    evaluator_lm = llama3_8b\n",
    "\n",
    "dspy.settings.configure(evaluator=evaluator_lm)\n",
    "\n",
    "# --- Cost Estimation (Approximation) ---\n",
    "# Based on OpenAI pricing for a capable model (e.g., gpt-4o-mini, $/1M tokens)\n",
    "COST_PER_INPUT_TOKEN = 0.15 / 1_000_000\n",
    "COST_PER_OUTPUT_TOKEN = 0.60 / 1_000_000\n",
    "\n",
    "def estimate_cost(input_text, output_text):\n",
    "    # A common heuristic: avg token length is ~4 chars\n",
    "    input_tokens = len(input_text) / 4\n",
    "    output_tokens = len(output_text) / 4\n",
    "    cost = (input_tokens * COST_PER_INPUT_TOKEN) + (output_tokens * COST_PER_OUTPUT_TOKEN)\n",
    "    return cost\n",
    "\n",
    "# --- Search Space Definition for Scikit-Optimize ---\n",
    "search_space = [\n",
    "    Categorical(list(models_map.keys()), name='model_name'),\n",
    "    Real(0.0, 1.0, name='temperature'),\n",
    "    Categorical([256, 512, 768], name='max_tokens')\n",
    "]\n",
    "\n",
    "print(\"\\nConfiguration complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Data, Signature, and Metrics\n",
    "\n",
    "We set up the summarization task structure, create a sample dataset, and define the core evaluation logic for our three objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DSPy Signature & Dataset ---\n",
    "class SummarizationSignature(dspy.Signature):\n",
    "    \"\"\"Summarize the given document into a short, concise paragraph.\"\"\"\n",
    "    document = dspy.InputField(desc=\"A long text document.\")\n",
    "    summary = dspy.OutputField(desc=\"A concise summary of the document.\")\n",
    "\n",
    "trainset = [\n",
    "    dspy.Example(document=\"Quantum computing is a revolutionary type of computing that leverages the principles of quantum mechanics to process information. Unlike classical computers, which use bits to represent information as either a 0 or a 1, quantum computers use qubits. Qubits can exist in a superposition of both 0 and 1 simultaneously, and they can be entangled, meaning their fates are linked even when physically separated. This allows quantum computers to perform complex calculations at speeds unattainable by classical computers. Key applications include drug discovery, materials science, financial modeling, and breaking cryptographic codes. However, building and maintaining stable quantum computers is a massive engineering challenge due to qubit decoherence, where qubits lose their quantum properties due to interaction with the environment.\").with_inputs('document'),\n",
    "    dspy.Example(document=\"The Roman Republic was a period of ancient Roman civilization that began with the overthrow of the Roman Kingdom in 509 BC and ended in 27 BC with the establishment of the Roman Empire. It was characterized by a republican form of government, where annual magistrates were elected by the citizens. The cornerstone of the Republic was the Senate, a body of elder statesmen that advised the magistrates. Society was divided into patricians (the aristocracy) and plebeians (the common citizens). For centuries, the Republic expanded its territory through conquest and alliances, eventually controlling the entire Mediterranean basin. However, internal tensions, civil wars, and the rise of powerful military leaders like Julius Caesar ultimately led to its collapse and the rise of Augustus as the first Roman Emperor.\").with_inputs('document')\n",
    "]\n",
    "\n",
    "# --- Evaluation Metrics ---\n",
    "accuracy_metric = dspy.Assess(\n",
    "    dev_data=trainset, \n",
    "    metric=lambda gold, pred, trace: dspy.evaluate.answer_exact_match(gold, pred, trace=trace) and dspy.evaluate.answer_faithfulness_score(gold, pred, trace=trace),\n",
    "    display_progress=False,\n",
    "    display_table=0,\n",
    ")\n",
    "\n",
    "def evaluate_configuration(model_name, temperature, max_tokens):\n",
    "    \"\"\"Runs a given configuration and returns the three objective scores.\"\"\"\n",
    "    # 1. Configure the LM\n",
    "    lm = models_map[model_name]\n",
    "    lm.config['temperature'] = temperature\n",
    "    lm.config['max_tokens'] = max_tokens\n",
    "    \n",
    "    with dspy.settings.context(lm=lm):\n",
    "        summarizer = dspy.Predict(SummarizationSignature)\n",
    "        \n",
    "        # 2. Evaluate Accuracy\n",
    "        # We'll use a simplified dspy.Assess for this programmatic evaluation\n",
    "        # A more robust approach would be a custom metric as in the previous notebook\n",
    "        accuracy_score = accuracy_metric(summarizer, dev_data=trainset)[0]\n",
    "\n",
    "        # 3. Evaluate Latency and Cost\n",
    "        total_latency = 0\n",
    "        total_cost = 0\n",
    "        for example in trainset:\n",
    "            start_time = time.time()\n",
    "            prediction = summarizer(document=example.document)\n",
    "            total_latency += (time.time() - start_time)\n",
    "            total_cost += estimate_cost(example.document, prediction.summary)\n",
    "            \n",
    "    avg_latency = total_latency / len(trainset)\n",
    "    avg_cost = total_cost / len(trainset)\n",
    "    \n",
    "    return accuracy_score, avg_latency, avg_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bayesian Optimization\n",
    "\n",
    "We define the objective function for the optimizer. Since Bayesian optimizers minimize a *single* value, we **scalarize** our three objectives into one score. A simple weighted sum is a common technique. The optimizer's goal is to find hyperparameters that minimize this combined score. We will store the results of every trial to build our Pareto frontier later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated_points = []\n",
    "iteration_count = 0\n",
    "\n",
    "@skopt.utils.use_named_args(search_space)\n",
    "def objective_function(**params):\n",
    "    global iteration_count\n",
    "    iteration_count += 1\n",
    "    \n",
    "    model_name = params['model_name']\n",
    "    temperature = params['temperature']\n",
    "    max_tokens = params['max_tokens']\n",
    "    \n",
    "    print(f\"[Iteration {iteration_count}] Testing: {model_name}, Temp: {temperature:.2f}, Max Tokens: {max_tokens}\")\n",
    "    \n",
    "    # Get our three objective scores\n",
    "    accuracy, latency, cost = evaluate_configuration(model_name, temperature, max_tokens)\n",
    "    print(f\"  -> Results: Accuracy={accuracy:.2f}, Latency={latency:.2f}s, Cost=${cost:.8f}\")\n",
    "    \n",
    "    # Store the raw results for Pareto analysis later\n",
    "    evaluated_points.append({\n",
    "        'model_name': model_name,\n",
    "        'temperature': temperature,\n",
    "        'max_tokens': max_tokens,\n",
    "        'accuracy': accuracy,\n",
    "        'latency': latency,\n",
    "        'cost': cost\n",
    "    })\n",
    "\n",
    "    # Scalarize the objectives into a single value to minimize.\n",
    "    # We want to MINIMIZE latency and cost, and MAXIMIZE accuracy.\n",
    "    # Therefore, we minimize (-1 * accuracy).\n",
    "    # Weights can be tuned to prioritize one objective over others.\n",
    "    # Let's start with a balanced approach.\n",
    "    weight_accuracy = 1.5 # Higher weight to prioritize quality\n",
    "    weight_latency = 1.0\n",
    "    weight_cost = 0.5\n",
    "    \n",
    "    # We normalize latency and cost to bring them to a similar scale as accuracy (0-100)\n",
    "    # Assuming max latency of ~30s and max cost of ~$0.0005 for normalization\n",
    "    norm_latency = latency / 30.0\n",
    "    norm_cost = cost / 0.0005\n",
    "    \n",
    "    score = (weight_latency * norm_latency) + (weight_cost * norm_cost) - (weight_accuracy * (accuracy / 100.0))\n",
    "    return score\n",
    "\n",
    "# --- Run the Optimizer ---\n",
    "N_CALLS = 25 # Number of configurations to test\n",
    "print(f\"\\nStarting Bayesian optimization for {N_CALLS} iterations...\\n\")\n",
    "\n",
    "results = gp_minimize(\n",
    "    func=objective_function, \n",
    "    dimensions=search_space, \n",
    "    n_calls=N_CALLS, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nOptimization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Identify Pareto Frontier and Visualize\n",
    "\n",
    "From all the configurations tested, we now identify the **Pareto-efficient** points. A point is on the Pareto frontier if you cannot improve one objective without worsening another. These represent the optimal trade-offs.\n",
    "\n",
    "We then plot these points in a 3D scatter plot to visualize the frontier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pareto_efficient(points):\n",
    "    \"\"\"\n",
    "    Finds the Pareto-efficient points from a set of points.\n",
    "    :param points: An (n_points, n_objectives) array.\n",
    "    :return: A (n_points, ) boolean array, indicating whether each point is Pareto efficient.\n",
    "    \"\"\"\n",
    "    is_efficient = np.ones(points.shape[0], dtype=bool)\n",
    "    for i, p in enumerate(points):\n",
    "        if is_efficient[i]:\n",
    "            # Find all points that are not p\n",
    "            other_points = np.arange(points.shape[0]) != i\n",
    "            # Find all points that are better or equal in all objectives\n",
    "            dominating_points = points[other_points]\n",
    "            is_dominated = np.any(np.all(dominating_points <= p, axis=1))\n",
    "            if is_dominated:\n",
    "                is_efficient[i] = False\n",
    "    return is_efficient\n",
    "\n",
    "# --- Prepare Data for Analysis ---\n",
    "df = pd.DataFrame(evaluated_points)\n",
    "\n",
    "# We want to minimize latency and cost, but maximize accuracy.\n",
    "# The Pareto function assumes all objectives are being minimized, so we use (-1 * accuracy).\n",
    "points_for_pareto = df[['latency', 'cost', 'accuracy']].copy()\n",
    "points_for_pareto['accuracy'] = -1 * points_for_pareto['accuracy']\n",
    "\n",
    "# --- Find and Save the Frontier ---\n",
    "pareto_mask = is_pareto_efficient(points_for_pareto.values)\n",
    "pareto_df = df[pareto_mask].sort_values(by='accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "output_filename = 'pareto_frontier.csv'\n",
    "pareto_df.to_csv(output_filename, index=False)\n",
    "print(f\"Pareto frontier saved to {output_filename}\")\n",
    "\n",
    "# --- Visualize the Results ---\n",
    "# Create hover text\n",
    "df['text'] = df.apply(lambda row: f\"Model: {row['model_name']}<br>Temp: {row['temperature']:.2f}<br>Max Tokens: {row['max_tokens']}<br>Accuracy: {row['accuracy']:.2f}<br>Latency: {row['latency']:.2f}s<br>Cost: ${row['cost']:.6f}\", axis=1)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add all evaluated points (non-Pareto)\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=df[~pareto_mask]['latency'],\n",
    "    y=df[~pareto_mask]['cost'],\n",
    "    z=df[~pareto_mask]['accuracy'],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color='blue', opacity=0.6),\n",
    "    text=df[~pareto_mask]['text'],\n",
    "    hoverinfo='text',\n",
    "    name='Dominated Points'\n",
    "))\n",
    "\n",
    "# Add Pareto-efficient points\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=df[pareto_mask]['latency'],\n",
    "    y=df[pareto_mask]['cost'],\n",
    "    z=df[pareto_mask]['accuracy'],\n",
    "    mode='markers',\n",
    "    marker=dict(size=8, color='red', symbol='diamond'),\n",
    "    text=df[pareto_mask]['text'],\n",
    "    hoverinfo='text',\n",
    "    name='Pareto Frontier'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Multi-Objective Optimization: Accuracy vs. Latency vs. Cost',\n",
    "    scene=dict(\n",
    "        xaxis_title='Latency (s) ↓',\n",
    "        yaxis_title='Cost ($) ↓',\n",
    "        zaxis_title='Accuracy (%) ↑'\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, b=0, t=40)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# --- Display the Pareto Frontier Table ---\n",
    "print(\"\\n--- Pareto Optimal Configurations ---\")\n",
    "print(pareto_df.to_markdown(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
